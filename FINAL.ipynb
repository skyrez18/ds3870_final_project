{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Car Sale Auction Prices\n",
    "By Sky Reznik, John Lackey and Kevin Abatto\n",
    "2025-04-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; # type: ignore\n",
    "import numpy as np; # type: ignore\n",
    "import seaborn as sns; # type: ignore\n",
    "import matplotlib.pyplot as plt; # type: ignore\n",
    "from holoviews.operation import histogram;\n",
    "from sklearn.preprocessing import OneHotEncoder; # type: ignore\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.linear_model import LinearRegression;\n",
    "from sklearn.metrics import mean_squared_error, r2_score;\n",
    "import os;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into a pandas dataframe\n",
    "shared_file_path = './kaggle_datasets/car_prices.csv'\n",
    "# line 408,163 - \"Model\" field contains a comma (SE PZEV w/Connectivity, Navigation) - specify quotechar='\"'\n",
    "# This tells Pandas to treat anything inside double quotes as a single field, even if it contains commas.\n",
    "#      solution provided by ChatGPT\n",
    "df = pd.read_csv(shared_file_path, quotechar='\"', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need at append the MSRP of these cars. By using a data set that has features: make, model, year, transmission, etc. we can match the values and add in the MSRP.\n",
    "\n",
    "We have excluded trim and chosen the lowest price match to avoid over saturating the data with multiple trim levels at different prices of the same vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "msrp = pd.read_csv('MSRP.csv')\n",
    "\n",
    "# Standardize column names and values\n",
    "msrp.rename(columns={\n",
    "    'Make': 'make',\n",
    "    \"Model\": \"model\",\n",
    "    \"Year\": \"year\",\n",
    "    \"Transmission Type\": \"transmission\"\n",
    "}, inplace=True)\n",
    "\n",
    "msrp['transmission'] = msrp['transmission'].str.lower()\n",
    "\n",
    "# Create matching keys\n",
    "df['match_key'] = (\n",
    "    df['year'].astype(str).str.lower() + '_' + \n",
    "    df['make'].str.lower() + '_' + \n",
    "    df['model'].str.lower() + '_' + \n",
    "    df['transmission'].str.lower()\n",
    ")\n",
    "\n",
    "msrp['match_key'] = (\n",
    "    msrp['year'].astype(str).str.lower() + '_' + \n",
    "    msrp['make'].str.lower() + '_' + \n",
    "    msrp['model'].str.lower() + '_' + \n",
    "    msrp['transmission'].str.lower()\n",
    ")\n",
    "\n",
    "# Sort by MSRP (ascending) and keep first (min) for each match_key\n",
    "msrp_min_row = msrp.sort_values('MSRP').drop_duplicates('match_key', keep='first')\n",
    "\n",
    "# Merge with original car data (keeping all car rows but adding MSRP where matched)\n",
    "car_with_msrp = df.merge(\n",
    "    msrp_min_row[['match_key', 'MSRP']],  # Only keep needed columns\n",
    "    on='match_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the temporary key column\n",
    "car_with_msrp = car_with_msrp.drop(columns=['match_key'])\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "car_with_msrp.to_csv('car_prices_with_msrp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "1) Remove any row with missing data with df.dropna()\n",
    "2) One-Hot encode ['transmission'] to ['automatic_trans'] 0 /1 (double check unique vals)\n",
    "3) Simplify ['body'] (collapse all 85 body types to 9 types)\n",
    "4) Convert ['saledate']\n",
    "5) Age of car (from ['saledate'])\n",
    "6) Drop negative age cars\n",
    "7) Drop unessecary or redundant columns\n",
    "8) Make 'condition' numeric\n",
    "9) Avoid outlier by using the 95th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use the car_prices_with_msrp DataFrame\n",
    "df = pd.read_csv('car_prices_with_msrp.csv', low_memory=False)\n",
    "# Remove 'Unamed: 16' column\n",
    "df.drop('Unnamed: 16', axis=1, inplace=True)\n",
    "df['odometer'] = np.where(df['odometer'] == 999999.0, np.nan, df['odometer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Remove rows that have missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - One-hot encode the 'transmission' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['transmission'].unique())\n",
    "df['auto_transmission'] = np.where(df['transmission'].str.contains('automatic', case=False), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - 'One-hot encode' the 'body' column (count 85 unique values --> reduce to 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['suv' 'sedan' 'convertible' 'coupe' 'wagon' 'hatchback' 'truck' 'minivan' 'van']\n",
    "print(df['body'].unique())\n",
    "df['body_type'] = np.nan\n",
    "df['body_type'] = np.where(df['body'].str.contains('minivan', case=False), 'minivan', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('sedan', case=False), 'sedan', df['body'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('wagon', case=False), 'wagon', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('coupe', case=False), 'coupe', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('koup', case=False), 'coupe', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('convertible', case=False), 'convertible', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('hatchback', case=False), 'hatchback', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains(r'\\bvan\\b', case=False), 'van', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('truck', case=False), 'truck', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('cab', case=False), 'truck', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('crew', case=False), 'truck', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('suv', case=False), 'suv', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('Minivan', case=False), 'minivan', df['body_type'])\n",
    "\n",
    "# Numerical mapping for body types\n",
    "size_mapping = {\n",
    "    'convertible': 0,\n",
    "    'coupe': 1,\n",
    "    'hatchback': 2,\n",
    "    'sedan': 3,\n",
    "    'wagon': 4,\n",
    "    'suv': 5,\n",
    "    'minivan': 6,\n",
    "    'truck': 7,\n",
    "    'van': 8\n",
    "}\n",
    "\n",
    "df['body_size'] = df['body_type'].map(size_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Converting 'saledate' to datetime (solution provided by ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle invalid or unexpected values in the 'saledate' column\n",
    "# Extract just the date part (e.g., \"Dec 16 2014\") before conversion\n",
    "df['saledate'] = pd.to_datetime(\n",
    "    df['saledate'].str.extract(r'(\\w{3} \\d{2} \\d{4})')[0], \n",
    "    format='%b %d %Y', \n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Create a new column 'car_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['car_age'] = np.where(df['saledate'].notna(), df['saledate'].dt.year - df['year'], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Some car ages are negative (which is not possible) because a 2015 model year can exist in 2014 and subsequently be sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rows with values less than 0, drop rows\n",
    "print(\"Number of negative car ages dropped: \", df[df['car_age'] < 0].shape[0])\n",
    "df.drop(df[df['car_age'] < 0].index, inplace=True)\n",
    "# Drop old columns 'transmission' & 'body'\n",
    "df.drop(['transmission', 'body'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 - Drop unessecary or redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['year', 'trim', 'vin', 'color', 'interior', 'saledate', 'body_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 - Make 'condition' numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'] = pd.to_numeric(df['condition'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 - Use only th 95th percentile of the data to avoid outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='sellingprice')\n",
    "threshold = df['sellingprice'].quantile(0.95)\n",
    "df = df[df['sellingprice'] <= threshold]\n",
    "sns.histplot(data=df, x='sellingprice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 - Creating 'sold_above_mmr'\n",
    "\n",
    "MMR is provided in the dataset... it is the \"Mannheim Market Report\", an estimation of a car's selling value, which is updated nightly, and trained on millions of auction transactions. For our auction data, we can assume it is a sellers goal to surpass the MMR in the auction.\n",
    "\n",
    "Therefore, a relevant one-hot-encoding would be if the `sellingprice` > `mmr`. While it is good practice to not have redundant columns; ones which can be inferred from the data, a binary column `sold_above_mmr` would be highly useful for #TODO geographical visualizations and determining which sellers or states outpreform their estimated MMR.\n",
    "\n",
    "We will create this variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure mmr and sellingprice are numeric\n",
    "df['mmr'] = pd.to_numeric(df['mmr'], errors='coerce')\n",
    "df['sellingprice'] = pd.to_numeric(df['sellingprice'], errors='coerce')\n",
    "df['sold_above_mmr'] = np.where(df['sellingprice'] > df['mmr'], 1, 0 )\n",
    "df[['sellingprice', 'mmr', 'sold_above_mmr']].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clean data to CSV\n",
    "df.to_csv('cleaned_car_prices.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the cleaned data\n",
    "df = pd.read_csv('cleaned_car_prices.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's learn more about our data now that is has been cleaned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Features Overview\n",
    "\n",
    "##### 1. make\n",
    "- **Description**: The manufacturer/brand of the vehicle\n",
    "- **Type**: Categorical (e.g., Toyota, Ford, Honda)\n",
    "- **Potential Use**: Grouping vehicles by brand for analysis\n",
    "\n",
    "##### 2. model\n",
    "- **Description**: The specific model name of the vehicle\n",
    "- **Type**: Categorical (e.g., Camry, F-150, Escape)\n",
    "- **Potential Use**: Detailed vehicle identification when combined with make\n",
    "\n",
    "##### 3. state\n",
    "- **Description**: The geographical state where the vehicle is located/registered\n",
    "- **Type**: Categorical (e.g., CA, TX, NY)\n",
    "- **Potential Use**: Regional price analysis or demand patterns\n",
    "\n",
    "##### 4. condition\n",
    "- **Description**: The physical/mechanical condition of the vehicle\n",
    "- **Type**: Numerical (0-5)\n",
    "- **Potential Use**: Key factor in pricing models\n",
    "\n",
    "##### 5. odometer\n",
    "- **Description**: The mileage reading showing how many miles the vehicle has traveled\n",
    "- **Type**: Numerical (continuous)\n",
    "- **Potential Use**: Strong predictor of vehicle value and wear\n",
    "\n",
    "##### 6. seller\n",
    "- **Description**: The party selling the vehicle \n",
    "- **Type**: Categorical\n",
    "- **Potential Use**: Analyzing price differences between seller types\n",
    "\n",
    "##### 7. mmr (Manheim Market Report)\n",
    "- **Description**: Wholesale market valuation of the vehicle caluculated and updated every 24 hours\n",
    "- **Type**: Numerical (currency USD)\n",
    "- **Potential Use**: Benchmark for comparing selling prices\n",
    "\n",
    "##### 8. sellingprice\n",
    "- **Description**: The actual auction sale price of the vehicle\n",
    "- **Type**: Numerical (currency USD)\n",
    "- **Potential Use**: Target variable for price prediction models\n",
    "\n",
    "##### 9. MSRP (Manufacturer's Suggested Retail Price)\n",
    "- **Description**: The original new vehicle price recommended by manufacturer\n",
    "- **Type**: Numerical (currency USD)\n",
    "- **Potential Use**: Baseline for depreciation calculations\n",
    "\n",
    "##### 10. auto_transmission\n",
    "- **Description**: Whether the vehicle has automatic transmission\n",
    "- **Type**: Boolean (0/1)\n",
    "- **Potential Use**: Analyzing price differences between transmission types\n",
    "\n",
    "##### 11. body_size\n",
    "- **Description**: The size classification of the vehicle\n",
    "- **Type**: Ordinal numerical (1=smallest to 8=largest)\n",
    "- **Potential Use**: Market segment analysis\n",
    "\n",
    "##### 12. car_age\n",
    "- **Description**: The age of the vehicle in years\n",
    "- **Type**: Numerical (discrete)\n",
    "- **Potential Use**: Key factor in depreciation models\n",
    "\n",
    "##### 13. sold_above_mmr\n",
    "- **Description**: The age of the vehicle in years\n",
    "- **Type**: Boolean (1/0)\n",
    "- **Potential Use**: Determining shortcomings of MMR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a look at the format of the dataframe and what a line look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots\n",
    "First, lets look at some bloxplots of our numerical features to understand the spread of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = ['odometer', 'condition', 'sellingprice', 'MSRP', 'car_age', 'body_size']\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(3, 2, i)  \n",
    "    sns.boxplot(y=df[col], color='skyblue')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()  # Prevent overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(\n",
    "    df[numerical_cols].corr(),\n",
    "    annot = True,\n",
    "    fmt = '.2f',\n",
    "    cmap = 'coolwarm_r',\n",
    "    vmin = -1, \n",
    "    vmax = 1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's do some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#https://geopandas.org/en/stable/docs/user_guide/mapping.html\n",
    "\n",
    "# Verify the installation by printing the version\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "\n",
    "gdf = gpd.read_file('tl_2024_us_state/tl_2024_us_state.shp') #read file from US census\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "df2 = pd.read_csv('cleaned_car_prices.csv')\n",
    "\n",
    "sales_counts = df2['state'].value_counts().reset_index()\n",
    "sales_counts.rename(columns={'state':'STUSPS'}, inplace=True) # rename for merge\n",
    "sales_counts['STUSPS'] = sales_counts['STUSPS'].str.upper() # uppercase for merge\n",
    "\n",
    "merged_df = gdf.merge(sales_counts, how='left', on='STUSPS')\n",
    "\n",
    "non_continental = ['HI','VI','MP','GU','AK','AS','PR']\n",
    "us49 = merged_df\n",
    "for n in non_continental:\n",
    "    us49 = us49[us49.STUSPS != n]\n",
    "\n",
    "# format and convert NaN's to zeroes\n",
    "us49.rename(columns={'count':'NUM_SALES'}, inplace=True)\n",
    "us49.fillna(0, inplace=True)\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.pyplot import colormaps\n",
    "\n",
    "# Source for function https://medium.com/@jl_ruiz/plot-maps-from-the-us-census-bureau-using-geopandas-and-contextily-in-python-df787647ef77\n",
    "def StatesPlot(df,data, cmap):\n",
    "    f,ax = plt.subplots(1,1, figsize=(15,10),\n",
    "    sharex=True, sharey=True, dpi=300)\n",
    "    f.tight_layout()\n",
    "    plt.title('Car sales per state')\n",
    "    ax.set_axis_off()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"4%\",\n",
    "    pad=0.1,alpha=0.01)\n",
    "    df.plot(data, ax=ax, alpha=0.5, cmap=cmap,\n",
    "    edgecolor='none', legend=True, cax=cax, linewidth=0.3)\n",
    "    plt.ylabel('Sales', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "StatesPlot(us49, 'NUM_SALES', cmap='Wistia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a majority of sales are California, Texas, and Florida. While it isn't relevant to other questions, we can guess that maybe this is due to limitations of the dataset, and simply state populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Sample data\n",
    "treemap_df = df2['make'].value_counts().reset_index()\n",
    "treemap_df.columns = ['make', 'count']\n",
    "\n",
    "make_to_country = {\n",
    "    \"Infiniti\": \"Japan\",\n",
    "    \"Dodge\": \"USA\",\n",
    "    \"Chevrolet\": \"USA\",\n",
    "    \"Chrysler\": \"USA\",\n",
    "    \"Kia\": \"South Korea\",\n",
    "    \"Ford\": \"USA\",\n",
    "    \"Lexus\": \"Japan\",\n",
    "    \"Pontiac\": \"USA\",\n",
    "    \"Nissan\": \"Japan\",\n",
    "    \"Hyundai\": \"South Korea\",\n",
    "    \"Acura\": \"Japan\",\n",
    "    \"GMC\": \"USA\",\n",
    "    \"Mazda\": \"Japan\",\n",
    "    \"Buick\": \"USA\",\n",
    "    \"Toyota\": \"Japan\",\n",
    "    \"Lincoln\": \"USA\",\n",
    "    \"Volkswagen\": \"Germany\",\n",
    "    \"Cadillac\": \"USA\",\n",
    "    \"Honda\": \"Japan\",\n",
    "    \"Suzuki\": \"Japan\",\n",
    "    \"Subaru\": \"Japan\",\n",
    "    \"Volvo\": \"Sweden\",\n",
    "    \"Mitsubishi\": \"Japan\",\n",
    "    \"Mercedes-Benz\": \"Germany\",\n",
    "    \"Scion\": \"Japan\",\n",
    "    \"Oldsmobile\": \"USA\",\n",
    "    \"BMW\": \"Germany\",\n",
    "    \"HUMMER\": \"USA\",\n",
    "    \"Land Rover\": \"UK\",\n",
    "    \"Saab\": \"Sweden\",\n",
    "    \"Audi\": \"Germany\",\n",
    "    \"Porsche\": \"Germany\",\n",
    "    \"Plymouth\": \"USA\",\n",
    "    \"FIAT\": \"Italy\",\n",
    "    \"Maserati\": \"Italy\"\n",
    "}\n",
    "\n",
    "treemap_df['country'] = treemap_df['make'].map(make_to_country)\n",
    "\n",
    "fig = px.treemap(\n",
    "    treemap_df,\n",
    "    path=['country','make'],\n",
    "    values='count',\n",
    "    color='country',\n",
    "color_discrete_sequence=px.colors.qualitative.Safe)\n",
    "\n",
    "fig.update_layout(margin=dict(t=30, l=10, r=10, b=10))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Predictions\n",
    "#### 'mmr' v. 'sellingprice'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.scatter(df['sellingprice'], df['mmr'], alpha=0.2, s=5)  \n",
    "sns.regplot(x='sellingprice', y='mmr', data=df, scatter=False, color='red')  \n",
    "plt.title('Selling Price vs MMR')  \n",
    "plt.xlabel('Selling Price')  \n",
    "plt.ylabel('MMR') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['mmr']  # Predictor \n",
    "y = df['sellingprice']  # Target \n",
    "r_squared = r2_score(y, X)\n",
    "print(f\"R-squared (R²) between MMR and Selling Price: {r_squared}\")\n",
    "\n",
    "\n",
    "# Create a DataFrame to store R² values\n",
    "results = pd.DataFrame(columns=['Predictor', 'R_Squared'])\n",
    "results.loc[0] = ['MMR', r2_score(df['sellingprice'], df['mmr'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can get a baseline understanding of how accurate MMR is. With an R² of .9538, it explains 95% of 'sellingprice' variance. We will see if we can create a model better than the industry standard! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using features to predict selling price with Linear Regression\n",
    "List of numeric features:\n",
    "1. year\n",
    "2. condition\n",
    "3. odometer\n",
    "4. auto_transmission\n",
    "5. car_age\n",
    "6. MSRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictor = df[['condition', 'odometer', 'auto_transmission', 'car_age', 'MSRP', 'body_size']]\n",
    "df_target = df['sellingprice']\n",
    "Xlr = df_predictor\n",
    "ylr = df_target\n",
    "\n",
    "# Split into training and testing set\n",
    "Xlr_train, Xlr_test, Ylr_train, Ylr_test = train_test_split(Xlr, ylr, test_size=0.3, random_state=39)\n",
    "# Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(Xlr_train, Ylr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylr_pred = model.predict(Xlr_test)\n",
    "mse = mean_squared_error(Ylr_test, ylr_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "r2_linreg = r2_score(Ylr_test, ylr_pred)\n",
    "print(f\"R^2 (coefficient of determination): {r2_linreg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, unsuprisingly, our linear regression model did not preform as well as MMR did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[1] = ['Linear Regression', r2_linreg]\n",
    "results.sort_values(by='R_Squared', ascending=False, inplace=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting predictions from linear regression model vs. actual selling price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(Ylr_test, ylr_pred, alpha=0.5, color='blue') \n",
    "plt.plot([min(Ylr_test), max(Ylr_test)], [min(Ylr_test), max(Ylr_test)], color='red', linewidth=1.5) # Perfect prediction line\n",
    "plt.xlabel('Actual Selling Price') \n",
    "plt.ylabel('Linear Regression Predicted Selling Price')\n",
    "plt.title('Actual vs Linear Regression Predicted Selling Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization and PCA\n",
    "\n",
    "To visualize using PCA, we must standardize the features using `StandardScaler()` from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['odometer', 'condition', 'MSRP', 'car_age']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Starting the engine\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_stand = pd.DataFrame(\n",
    "    scaler.fit_transform(df[numerical_cols]),\n",
    "    # Keeping the index and \n",
    "    index = df.index,\n",
    "    columns = df[numerical_cols].columns\n",
    ")\n",
    "\n",
    "df_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "df_pca = PCA(n_components = 2).fit(df_stand)\n",
    "\n",
    "print(f'The variance retained by the first PC is: {round(df_pca.explained_variance_ratio_[0]*100, 2)}%')\n",
    "print(f'The variance retained by the second PC is: {round(df_pca.explained_variance_ratio_[1]*100, 2)}%')\n",
    "print(f'for a total variance retained of: {round(sum(df_pca.explained_variance_ratio_)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: 19_PCA.ipynb\n",
    "\n",
    "# Get PCA scores (coordinates of observations in PC space)\n",
    "scores = df_pca.transform(df_stand)\n",
    "\n",
    "# Get PCA loadings (contributions of original features to PCs)\n",
    "loadings = df_pca.components_.T * np.sqrt(df_pca.explained_variance_)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    x=scores[:, 0],  # PC1 scores\n",
    "    y=scores[:, 1],  # PC2 scores\n",
    "    alpha=0.5,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "for i, feature in enumerate(numerical_cols):\n",
    "    plt.arrow(\n",
    "        x=0, y=0,\n",
    "        dx=loadings[i, 0]*3,  # Scaling factor for visibility\n",
    "        dy=loadings[i, 1]*3,\n",
    "        color='red',\n",
    "        alpha=0.8,\n",
    "        head_width=0.1\n",
    "    )\n",
    "    plt.text(       # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n",
    "        loadings[i, 0]*3.6,  \n",
    "        loadings[-i, 1]*3.2,\n",
    "        feature,\n",
    "        horizontalalignment='center',  \n",
    "        color='red',\n",
    "        fontsize=12\n",
    "    )\n",
    "\n",
    "plt.title('PCA Biplot', fontsize=14)\n",
    "plt.xlabel(f'PC1 ({round(df_pca.explained_variance_ratio_[0]*100, 1)}% Variance)')\n",
    "plt.ylabel(f'PC2 ({round(df_pca.explained_variance_ratio_[1]*100, 1)}% Variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(0, color='grey', ls='--')\n",
    "plt.axvline(0, color='grey', ls='--')\n",
    "\n",
    "# Add variance explanation in legend - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
    "plt.legend(\n",
    "    handles=[plt.Line2D([0], [0], marker='o', color='w', label=f'Total Variance Explained: {round(sum(df_pca.explained_variance_ratio_)*100, 1)}%',\n",
    "             markerfacecolor='blue', markersize=10)],\n",
    "    loc='best'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the first two PC's can preform when predicting 'sellingprice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = df_pca.transform(df_stand)\n",
    "\n",
    "y_pca = df['sellingprice']\n",
    "X_pca = X_pca[:, [0, 1]]\n",
    "\n",
    "X_pca_train, X_pca_test, Y_pca_train, Y_pca_test = train_test_split(X_pca, y_pca, test_size=0.3, random_state=39)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_pca[:, [0, 1]], y_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pca_pred = model.predict(X_pca_test)\n",
    "mse = mean_squared_error(Y_pca_test, y_pca_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "r2_pca2 = r2_score(Y_pca_test, y_pca_pred)\n",
    "print(f\"R^2 (coefficient of determination): {r2_pca2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(Y_pca_test, y_pca_pred, alpha=0.5, color='red') \n",
    "plt.plot([min(Y_pca_test), max(Y_pca_test)], [min(Y_pca_test), max(Y_pca_test)], color='blue', linewidth=1.5) # Perfect prediction line\n",
    "plt.xlabel('Actual Selling Price') \n",
    "plt.ylabel('Two Principle Component Predicted Selling Price')\n",
    "plt.title('Actual vs Two Principle Component Predicted Selling Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and calculate R²\n",
    "y_pred = model.predict(X_pca[:, [0, 1]])\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "results.loc[2] = ['PCA (2 pricipal components)', r_squared]\n",
    "results.sort_values(by='R_Squared', ascending=False, inplace=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, it preforms pretty similar to the linear regression did with all the features, but it is impreessive how little it lost when moving to a 2-D reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "\n",
    "Let's see if XGBoost can do any better, first without tuning paremeters to find a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_predictor, df_target, test_size=0.3, random_state=39) # Create training set\n",
    "\n",
    "# Materials sourced from lecture slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_total = (\n",
    "    XGBRegressor(\n",
    "        max_depth = 5, \n",
    "        n_estimators = 200,\n",
    "    )\n",
    "    .fit(X_train, y_train)\n",
    ")\n",
    "\n",
    "# Calculating the R-squared using the test data\n",
    "xgb_R2 = xgb_total.score(X_test, y_test)\n",
    "\n",
    "print(f'R-squared value before tuning: {xgb_R2}')\n",
    "results.loc[3] = ['XGBoost (pretuned)', xgb_R2  ]\n",
    "# Sort results by R-squared values\n",
    "results.sort_values(by='R_Squared', ascending=False, inplace=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conserve on time & precious computing power for the grid search, let's see if there could be an earlier stopping point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_esr50 = (\n",
    "    XGBRegressor(\n",
    "        n_estimators = 1000,  # Create a thousand trees\n",
    "        learning_rate = 0.1,  # Learning rate\n",
    "        random_state = 3870,\n",
    "        early_stopping_rounds = 50  # Checks to see if there is an improvement 50 trees previously\n",
    "    )\n",
    "    .fit(\n",
    "        X_train, y_train,                # Training set for the model\n",
    "        eval_set = [(X_train, y_train),  # Calculates the rmse for the training set\n",
    "                    (X_test, y_test)],   # Calculates the rmse for the testing set\n",
    "        verbose = False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Saving the validation results\n",
    "xgb_esr50_df = pd.DataFrame({\n",
    "    'train error': xgbr_esr50.evals_result()['validation_0']['rmse'],\n",
    "    'test error':  xgbr_esr50.evals_result()['validation_1']['rmse']\n",
    "})\n",
    "\n",
    "print(xgb_esr50_df.nsmallest(n = 1, columns= 'test error').reset_index())\n",
    "\n",
    "# # Plotting the validation results:\n",
    "# xgb_esr50_df.plot(kind = 'line')\n",
    "\n",
    "# # plt.axvline(, color = 'black', linestyle = '--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, XGBoost has many hyper parameters. Let's do a grid search to find the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hp_dict = {\n",
    "    'learning_rate':    [0.01, 0.05, 0.1, 0.2, 0.3], # Each new prediction's contribution to y_hat\n",
    "    'n_estimators':     [100, 250, 500, 750, 1000],  # Number of trees to make\n",
    "    'max_depth':        [1, 3, 5, 7],                # Maximum number of splits: 1, 8, 32, 128 leaf nodes   \n",
    "    'colsample_bytree': [0.4, 0.6, 0.8, 1],          # Number of features to use per tree (40, 60, 80, & 100%)\n",
    "    'lambda':           [0, 0.5, 1, 2, 5]            # Regularization parameters to try\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# 5 fold cross-val\n",
    "cv5 = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "# Defining the model:\n",
    "xgb_reg = XGBRegressor(random_state = 3870, early_stopping_rounds = 50)\n",
    "\n",
    "# Defining the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = xgb_reg,\n",
    "    param_grid = xgb_hp_dict,\n",
    "    n_jobs = -1,\n",
    "    cv = cv5    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "curr_time = time.time()\n",
    "gs_results = (\n",
    "    grid_search\n",
    "    .fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_test, y_test)],\n",
    "        verbose = False\n",
    "    )\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Wow! That took {end_time - curr_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = grid_search.best_params_\n",
    "print(f'After many iterations... the best paremeters are')\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hard work is done, let's use these hyperparemeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = (\n",
    "    XGBRegressor(\n",
    "        **best_hps,\n",
    "    )\n",
    "    .fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_test, y_test)],\n",
    "        verbose = False\n",
    "    )\n",
    ")\n",
    "\n",
    "xgb_tuned_r2 = best_xgb.score(X_test, y_test)\n",
    "results.loc[4] = ['XGB (tuned)', xgb_tuned_r2]\n",
    "print(results.sort_values(by = 'R_Squared', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The r-squared is higher than MMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgboost.plot_importance(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "First let's import the proper packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to select the same predictor features as before and train-test split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictor = df[['condition', 'odometer', 'auto_transmission', 'car_age', 'MSRP', 'body_size']]\n",
    "df_target = df['sellingprice']\n",
    "\n",
    "X_rf_train, X_rf_test, Y_rf_train, Y_rf_test = train_test_split(df_predictor, df_target, test_size=0.3, random_state=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=31)\n",
    "rf_model.fit(X_rf_train, Y_rf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can make predicitons and look at the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_rf_pred = rf_model.predict(X_rf_test)\n",
    "\n",
    "mse_rf = mean_squared_error(Y_rf_test, Y_rf_pred)\n",
    "r2_rf = r2_score(Y_rf_test, Y_rf_pred)\n",
    "\n",
    "print(f\"Random Forest MSE: {mse_rf}\")\n",
    "print(f\"Random Forest R^2: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Model gives us a very strong R^2 value. Let's visualize the actual vs predicted values next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(Y_rf_test, Y_rf_pred, alpha=0.5, color='blue')\n",
    "plt.plot([min(Y_rf_test), max(Y_rf_test)], [min(Y_rf_test), max(Y_rf_test)], color='red', linewidth=1.0) \n",
    "plt.ylabel('Random Forest Predicted Selling Price')\n",
    "plt.title('Actual vs Random Forest Predicted Selling Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "We can also investigate what features were the most important in the splits of the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame({\n",
    "    'feature': df_predictor.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp.set_index('feature').plot(kind = 'bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the age of the car was the most important feature to the splitting of the trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20,],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=31)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=3, n_jobs=-1, verbose=1, scoring='r2')\n",
    "\n",
    "grid_search.fit(X_rf_train, Y_rf_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to find the best predictor and fit the random forests to the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "Y_rf_pred = best_rf.predict(X_rf_test)\n",
    "\n",
    "mse_rf = mean_squared_error(Y_rf_test, Y_rf_pred)\n",
    "r2_rf = r2_score(Y_rf_test, Y_rf_pred)\n",
    "\n",
    "print(f\"Tuned Random Forest MSE: {mse_rf}\")\n",
    "print(f\"Tuned Random Forest R^2: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a slight improvement in the tuned model vs the non-tuned model. Let's take a look at the feature importance again to see if there is much difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp2 = pd.DataFrame({\n",
    "    'feature': df_predictor.columns,\n",
    "    'importance': best_rf.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "feat_imp2.set_index('feature').plot(kind='bar', title='Feature Importance', figsize=(8,5))\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "feat_imp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is much more importance for odometer and MSRP variables compared to the previous Random Forest and less on the car age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[5] = ['Random Forest Regression', r2_rf]\n",
    "results.sort_values(by='R_Squared', ascending=False, inplace=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO What these R-squared values mean is that MMR is the best prediciton for sales price. This makes sense due to MMR being a wholesale market valuation of the car. However, the best model we have built so far is the Random Forest with the highest R-squared value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusion\n",
    "\n",
    "To compare the models, let's plot the r-squared of all them. We will not be able to plot PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() # retrain LR\n",
    "model.fit(Xlr_train, Ylr_train)\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_lr = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test, y_pred_xgb, label='XGBoost (tuned)', alpha=1, color='darkred')\n",
    "plt.scatter(y_test, y_pred_rf, label='Random forest', alpha=0.3, color='orange')\n",
    "plt.scatter(y_test, y_pred_lr, label='Linear regression', alpha=0.1, color='yellow')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k-', lw=1)\n",
    "plt.xlabel('Actual selling price')\n",
    "plt.ylabel('Predicted selling price')\n",
    "plt.grid(False)\n",
    "plt.legend()\n",
    "plt.ylim([-5000, y_test.max()])\n",
    "plt.show()\n",
    "\n",
    "#TODO: Should we use a ylim or not for visualization? Linear regression has many below 0\n",
    "#TODO: add label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/how-to-generate-feature-importance-plots-from-scikit-learn/\n",
    "feature_names = df_predictor.columns.drop('mmr')\n",
    "\n",
    "feature_imps = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'random forest': rf_model.feature_importances_,\n",
    "    'xgboost': best_xgb.feature_importances_,\n",
    "})\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "\n",
    "width = 0.35\n",
    "plt.xticks(x, feature_names, rotation=45)\n",
    "plt.bar(x - width/2, feature_imps['random forest'], width, label='Random forest', alpha=0.7, color='darkgreen')\n",
    "plt.bar(x + width/2, feature_imps['xgboost'],width, label='XGBoost', alpha=0.4, color='green')\n",
    "plt.legend()\n",
    "plt.ylabel('Feature importance')\n",
    "plt.show()\n",
    "\n",
    "#TODO: add label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
