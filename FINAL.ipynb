{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Car Sale Auction Prices\n",
    "By Sky Reznik, John Lackey and Kevin Abatto\n",
    "2025-04-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd; # type: ignore\n",
    "import numpy as np; # type: ignore\n",
    "import seaborn as sns; # type: ignore\n",
    "import matplotlib.pyplot as plt; # type: ignore\n",
    "from holoviews.operation import histogram;\n",
    "from sklearn.preprocessing import OneHotEncoder; # type: ignore\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.linear_model import LinearRegression;\n",
    "from sklearn.metrics import mean_squared_error, r2_score;\n",
    "import os;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load CSV into a pandas dataframe\n",
    "shared_file_path = './kaggle_datasets/car_prices.csv'\n",
    "# line 408,163 - \"Model\" field contains a comma (SE PZEV w/Connectivity, Navigation) - specify quotechar='\"'\n",
    "# This tells Pandas to treat anything inside double quotes as a single field, even if it contains commas.\n",
    "#      solution provided by ChatGPT\n",
    "df = pd.read_csv(shared_file_path, quotechar='\"', on_bad_lines='skip')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need at append the MSRP of these cars. By using a data set that had make, model, year, transmission, etc. we can match the values and add in the MSRP.\n",
    "\n",
    "We have excluded trim and chosen the lowest price match to avoid over saturating the data with multiple trim levels at different prices of the same vehicle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "msrp = pd.read_csv('MSRP.csv')\n",
    "\n",
    "# Standardize column names and values\n",
    "msrp.rename(columns={\n",
    "    'Make': 'make',\n",
    "    \"Model\": \"model\",\n",
    "    \"Year\": \"year\",\n",
    "    \"Transmission Type\": \"transmission\"\n",
    "}, inplace=True)\n",
    "\n",
    "msrp['transmission'] = msrp['transmission'].str.lower()\n",
    "\n",
    "# Create matching keys\n",
    "df['match_key'] = (\n",
    "    df['year'].astype(str).str.lower() + '_' + \n",
    "    df['make'].str.lower() + '_' + \n",
    "    df['model'].str.lower() + '_' + \n",
    "    df['transmission'].str.lower()\n",
    ")\n",
    "\n",
    "msrp['match_key'] = (\n",
    "    msrp['year'].astype(str).str.lower() + '_' + \n",
    "    msrp['make'].str.lower() + '_' + \n",
    "    msrp['model'].str.lower() + '_' + \n",
    "    msrp['transmission'].str.lower()\n",
    ")\n",
    "\n",
    "# Sort by MSRP (ascending) and keep first (min) for each match_key\n",
    "msrp_min_row = msrp.sort_values('MSRP').drop_duplicates('match_key', keep='first')\n",
    "\n",
    "# Merge with original car data (keeping all car rows but adding MSRP where matched)\n",
    "car_with_msrp = df.merge(\n",
    "    msrp_min_row[['match_key', 'MSRP']],  # Only keep needed columns\n",
    "    on='match_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the temporary key column\n",
    "car_with_msrp = car_with_msrp.drop(columns=['match_key'])\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "car_with_msrp.to_csv('car_prices_with_msrp.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "1) Remove any row with missing data with df.dropna()\n",
    "2) One-Hot encode ['transmission'] to ['automatic_trans'] 0 /1 (double check unique vals)\n",
    "3) Simplify ['body'] (collapse all 85 body types to 9 types)\n",
    "4) Convert ['saledate']\n",
    "5) Age of car (from ['saledate'])\n",
    "6) Drop negative age cars\n",
    "7) Drop unessecary or redundant columns\n",
    "8) Make 'condition' numeric\n",
    "9) Avoid outlier by using the 95th percentile"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# now we can use the car_prices_with_msrp DataFrame\n",
    "df = pd.read_csv('car_prices_with_msrp.csv', low_memory=False)\n",
    "# Remove 'Unamed: 16' column\n",
    "df.drop('Unnamed: 16', axis=1, inplace=True)\n",
    "df['odometer'] = np.where(df['odometer'] == 999999.0, np.nan, df['odometer'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Remove rows that have missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.dropna(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - One-hot encode the 'transmission' column\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df['transmission'].unique())\n",
    "df['auto_transmission'] = np.where(df['transmission'].str.contains('automatic', case=False), 1, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - 'One-hot encode' the 'body' column (count 85 unique values --> reduce to 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ['suv' 'sedan' 'convertible' 'coupe' 'wagon' 'hatchback' 'truck' 'minivan' 'van']\n",
    "df['body_type'] = np.nan\n",
    "df['body_type'] = np.where(df['body'].str.contains('minivan', case=False), 'minivan', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('sedan', case=False), 'sedan', df['body'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('wagon', case=False), 'wagon', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('coupe', case=False), 'coupe', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('koup', case=False), 'coupe', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('convertible', case=False), 'convertible', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('hatchback', case=False), 'hatchback', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains(r'\\bvan\\b', case=False), 'van', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('truck', case=False), 'truck', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('cab', case=False), 'truck', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('crew', case=False), 'truck', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('suv', case=False), 'suv', df['body_type'])\n",
    "df['body_type'] = np.where(df['body'].str.contains('Minivan', case=False), 'minivan', df['body_type'])\n",
    "\n",
    "# Numerical mapping for body types\n",
    "size_mapping = {\n",
    "    'convertible': 0,\n",
    "    'coupe': 1,\n",
    "    'hatchback': 2,\n",
    "    'sedan': 3,\n",
    "    'wagon': 4,\n",
    "    'suv': 5,\n",
    "    'minivan': 6,\n",
    "    'truck': 7,\n",
    "    'van': 8\n",
    "}\n",
    "\n",
    "df['body_size'] = df['body_type'].map(size_mapping)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Converting 'saledate' to datetime (solution provided by ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Handle invalid or unexpected values in the 'saledate' column\n",
    "# Extract just the date part (e.g., \"Dec 16 2014\") before conversion\n",
    "df['saledate'] = pd.to_datetime(\n",
    "    df['saledate'].str.extract(r'(\\w{3} \\d{2} \\d{4})')[0], \n",
    "    format='%b %d %Y', \n",
    "    errors='coerce'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Create a new column 'car_age'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['car_age'] = np.where(df['saledate'].notna(), df['saledate'].dt.year - df['year'], np.nan)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 - Some car ages are negative (which is not possible) because a 2015 model year can exist in 2014 and subsequently be sold"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# For rows with values less than 0, drop rows\n",
    "print(\"Number of negative car ages dropped: \", df[df['car_age'] < 0].shape[0])\n",
    "df.drop(df[df['car_age'] < 0].index, inplace=True)\n",
    "# Drop old columns 'transmission' & 'body'\n",
    "df.drop(['transmission', 'body'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 - Drop unessecary or redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.drop(['year', 'trim', 'vin', 'color', 'interior', 'saledate', 'body_type'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 - Make 'condition' numeric"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['condition'] = pd.to_numeric(df['condition'], errors='coerce')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 - Use only th 95th percentile of the data to avoid outliers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.histplot(data=df, x='sellingprice')\n",
    "threshold = df['sellingprice'].quantile(0.95)\n",
    "df = df[df['sellingprice'] <= threshold]\n",
    "sns.histplot(data=df, x='sellingprice')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 - Creating 'sold_above_mmr'\n",
    "\n",
    "MMR is provided in the dataset... it is the \"Mannheim Market Report\", an estimation of a car's selling value, which is updated nightly, and trained on millions of auction transactions. For our auction data, we can assume it is a sellers goal to surpass the MMR in the auction.\n",
    "\n",
    "Therefore, a relevant one-hot-encoding would be if the `sellingprice` > `mmr`. While it is good practice to not have redundant columns; ones which can be inferred from the data, a binary column `sold_above_mmr` would be highly useful for #TODO geographical visualizations and determining which sellers or states outpreform their estimated MMR.\n",
    "\n",
    "We will create this variable below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ensure mmr and sellingprice are numeric\n",
    "df['mmr'] = pd.to_numeric(df['mmr'], errors='coerce')\n",
    "df['sellingprice'] = pd.to_numeric(df['sellingprice'], errors='coerce')\n",
    "df['sold_above_mmr'] = np.where(df['sellingprice'] > df['mmr'], 1, 0 )\n",
    "df[['sellingprice', 'mmr', 'sold_above_mmr']].sample(4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save clean data to CSV\n",
    "df.to_csv('cleaned_car_prices.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Re-load the cleaned data\n",
    "df = pd.read_csv('cleaned_car_prices.csv', low_memory=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's learn more about our data now that is has been cleaned!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df.columns.unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Features Overview\n",
    "\n",
    "##### 1. make\n",
    "- **Description**: The manufacturer/brand of the vehicle\n",
    "- **Type**: Categorical (e.g., Toyota, Ford, Honda)\n",
    "- **Potential Use**: Grouping vehicles by brand for analysis\n",
    "\n",
    "##### 2. model\n",
    "- **Description**: The specific model name of the vehicle\n",
    "- **Type**: Categorical (e.g., Camry, F-150, Escape)\n",
    "- **Potential Use**: Detailed vehicle identification when combined with make\n",
    "\n",
    "##### 3. state\n",
    "- **Description**: The geographical state where the vehicle is located/registered\n",
    "- **Type**: Categorical (e.g., CA, TX, NY)\n",
    "- **Potential Use**: Regional price analysis or demand patterns\n",
    "\n",
    "##### 4. condition\n",
    "- **Description**: The physical/mechanical condition of the vehicle\n",
    "- **Type**: Numerical (0-5)\n",
    "- **Potential Use**: Key factor in pricing models\n",
    "\n",
    "##### 5. odometer\n",
    "- **Description**: The mileage reading showing how many miles the vehicle has traveled\n",
    "- **Type**: Numerical (continuous)\n",
    "- **Potential Use**: Strong predictor of vehicle value and wear\n",
    "\n",
    "##### 6. seller\n",
    "- **Description**: The party selling the vehicle \n",
    "- **Type**: Categorical\n",
    "- **Potential Use**: Analyzing price differences between seller types\n",
    "\n",
    "##### 7. mmr (Manheim Market Report)\n",
    "- **Description**: Wholesale market valuation of the vehicle caluculated and updated every 24 hours\n",
    "- **Type**: Numerical (currency USD)\n",
    "- **Potential Use**: Benchmark for comparing selling prices\n",
    "\n",
    "##### 8. sellingprice\n",
    "- **Description**: The actual auction sale price of the vehicle\n",
    "- **Type**: Numerical (currency USD)\n",
    "- **Potential Use**: Target variable for price prediction models\n",
    "\n",
    "##### 9. MSRP (Manufacturer's Suggested Retail Price)\n",
    "- **Description**: The original new vehicle price recommended by manufacturer\n",
    "- **Type**: Numerical (currency USD)\n",
    "- **Potential Use**: Baseline for depreciation calculations\n",
    "\n",
    "##### 10. auto_transmission\n",
    "- **Description**: Whether the vehicle has automatic transmission\n",
    "- **Type**: Boolean (0/1)\n",
    "- **Potential Use**: Analyzing price differences between transmission types\n",
    "\n",
    "##### 11. body_size\n",
    "- **Description**: The size classification of the vehicle\n",
    "- **Type**: Ordinal numerical (1=smallest to 8=largest)\n",
    "- **Potential Use**: Market segment analysis\n",
    "\n",
    "##### 12. car_age\n",
    "- **Description**: The age of the vehicle in years\n",
    "- **Type**: Numerical (discrete)\n",
    "- **Potential Use**: Key factor in depreciation models\n",
    "\n",
    "##### 13. sold_above_mmr\n",
    "- **Description**: The age of the vehicle in years\n",
    "- **Type**: Boolean (1/0)\n",
    "- **Potential Use**: Determining shortcomings of MMR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a look at the format of the dataframe and what a line look like:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df.sample(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots\n",
    "First, lets look at some bloxplots of our numerical features to understand the spread of our data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = ['odometer', 'condition', 'mmr', 'sellingprice', 'MSRP', 'car_age', 'body_size']\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(3, 3, i)  \n",
    "    sns.boxplot(y=df[col], color='skyblue')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()  # Prevent overlapping\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(\n",
    "    df[numerical_cols].corr(),\n",
    "    annot = True,\n",
    "    fmt = '.2f',\n",
    "    cmap = 'coolwarm_r',\n",
    "    vmin = -1, \n",
    "    vmax = 1\n",
    ");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Predictions\n",
    "#### 'mmr' v. 'sellingprice'\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.scatter(df['sellingprice'], df['mmr'], alpha=0.2, s=5)  \n",
    "sns.regplot(x='sellingprice', y='mmr', data=df, scatter=False, color='red')  \n",
    "plt.title('Selling Price vs MMR')  \n",
    "plt.xlabel('Selling Price')  \n",
    "plt.ylabel('MMR') \n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X = df['mmr']  # Predictor (independent variable)\n",
    "y = df['sellingprice']  # Target (dependent variable)\n",
    "r_squared = r2_score(y, X)\n",
    "print(f\"R-squared (R²) between MMR and Selling Price: {r_squared}\")\n",
    "\n",
    "\n",
    "# Create a DataFrame to store R² values\n",
    "results = pd.DataFrame(columns=['Predictor', 'R_Squared'])\n",
    "results.loc[0] = ['mmr', r2_score(df['sellingprice'], df['mmr'])]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can get a baseline understanding of how accurate MMR is. With an R² of .9538, it explains 95% of 'sellingprice' variance. We will see if we can create a model better than the industry standard! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using features to predict selling price with Linear Regression\n",
    "List of numeric features:\n",
    "1. year\n",
    "2. condition\n",
    "3. odometer\n",
    "4. auto_transmission\n",
    "5. car_age\n",
    "6. MSRP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_predictor = df[['condition', 'odometer', 'auto_transmission', 'car_age', 'MSRP', 'body_size']]\n",
    "df_target = df['sellingprice']\n",
    "Xlr = df_predictor\n",
    "ylr = df_target\n",
    "\n",
    "# Split into training and testing set\n",
    "Xlr_train, Xlr_test, Ylr_train, Ylr_test = train_test_split(Xlr, ylr, test_size=0.3, random_state=39)\n",
    "# Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(Xlr_train, Ylr_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ylr_pred = model.predict(Xlr_test)\n",
    "mse = mean_squared_error(Ylr_test, ylr_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "r2_linreg = r2_score(Ylr_test, ylr_pred)\n",
    "print(f\"R^2 (coefficient of determination): {r2_linreg}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, unsuprisingly, our linear regression model did not preform as well as MMR did."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results.loc[1] = ['Linear Regression', r2_linreg]\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting predictions from linear regression model vs. actual selling price"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(Ylr_test, ylr_pred, alpha=0.5, color='blue') \n",
    "plt.plot([min(Ylr_test), max(Ylr_test)], [min(Ylr_test), max(Ylr_test)], color='red', linewidth=1.5) # Perfect prediction line\n",
    "plt.xlabel('Actual Selling Price') \n",
    "plt.ylabel('Linear Regression Predicted Selling Price')\n",
    "plt.title('Actual vs Linear Regression Predicted Selling Price')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization and PCA\n",
    "\n",
    "To visualize using PCA, we must standardize the features using `StandardScaler()` from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_cols = ['odometer', 'condition', 'MSRP', 'car_age']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Starting the engine\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_stand = pd.DataFrame(\n",
    "    scaler.fit_transform(df[numerical_cols]),\n",
    "    # Keeping the index and \n",
    "    index = df.index,\n",
    "    columns = df[numerical_cols].columns\n",
    ")\n",
    "\n",
    "df_stand"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "df_pca = PCA(n_components = 2).fit(df_stand)\n",
    "\n",
    "print(f'The variance retained by the first PC is: {round(df_pca.explained_variance_ratio_[0]*100, 2)}%')\n",
    "print(f'The variance retained by the second PC is: {round(df_pca.explained_variance_ratio_[1]*100, 2)}%')\n",
    "print(f'for a total variance retained of: {round(sum(df_pca.explained_variance_ratio_)*100, 2)}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Source: 19_PCA.ipynb\n",
    "\n",
    "# Get PCA scores (coordinates of observations in PC space)\n",
    "scores = df_pca.transform(df_stand)\n",
    "\n",
    "# Get PCA loadings (contributions of original features to PCs)\n",
    "loadings = df_pca.components_.T * np.sqrt(df_pca.explained_variance_)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    x=scores[:, 0],  # PC1 scores\n",
    "    y=scores[:, 1],  # PC2 scores\n",
    "    alpha=0.5,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "for i, feature in enumerate(numerical_cols):\n",
    "    plt.arrow(\n",
    "        x=0, y=0,\n",
    "        dx=loadings[i, 0]*3,  # Scaling factor for visibility\n",
    "        dy=loadings[i, 1]*3,\n",
    "        color='red',\n",
    "        alpha=0.8,\n",
    "        head_width=0.1\n",
    "    )\n",
    "    plt.text(\n",
    "        loadings[i, 0]*3.2,  # Slightly offset from arrow\n",
    "        loadings[i, 1]*3.2,\n",
    "        feature,\n",
    "        color='darkred',\n",
    "        fontsize=12\n",
    "    )\n",
    "\n",
    "plt.title('PCA Biplot', fontsize=14)\n",
    "plt.xlabel(f'PC1 ({round(df_pca.explained_variance_ratio_[0]*100, 1)}% Variance)')\n",
    "plt.ylabel(f'PC2 ({round(df_pca.explained_variance_ratio_[1]*100, 1)}% Variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(0, color='grey', ls='--')\n",
    "plt.axvline(0, color='grey', ls='--')\n",
    "\n",
    "# Add variance explanation in legend\n",
    "plt.legend(\n",
    "    handles=[plt.Line2D([0], [0], marker='o', color='w', label=f'Total Variance Explained: {round(sum(df_pca.explained_variance_ratio_)*100, 1)}%',\n",
    "             markerfacecolor='blue', markersize=10)],\n",
    "    loc='best'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the first two PC's can preform when predicing 'sellingprice'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_pca = df_pca.transform(df_stand)\n",
    "\n",
    "y_pca = df['sellingprice']\n",
    "X_pca = X_pca[:, [0, 1]]\n",
    "\n",
    "X_pca_train, X_pca_test, Y_pca_train, Y_pca_test = train_test_split(X_pca, y_pca, test_size=0.3, random_state=39)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_pca[:, [0, 1]], y_pca)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_pca_pred = model.predict(X_pca_test)\n",
    "mse = mean_squared_error(Y_pca_test, y_pca_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "r2_pca2 = r2_score(Y_pca_test, y_pca_pred)\n",
    "print(f\"R^2 (coefficient of determination): {r2_pca2}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(Y_pca_test, y_pca_pred, alpha=0.5, color='red') \n",
    "plt.plot([min(Y_pca_test), max(Y_pca_test)], [min(Y_pca_test), max(Y_pca_test)], color='blue', linewidth=1.5) # Perfect prediction line\n",
    "plt.xlabel('Actual Selling Price') \n",
    "plt.ylabel('Two Principle Component Predicted Selling Price')\n",
    "plt.title('Actual vs Two Principle Component Predicted Selling Price')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predict and calculate R²\n",
    "y_pred = model.predict(X_pca[:, [0, 1]])\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "results.loc[2] = ['PCA (2 pricipal components)', r_squared]\n",
    "print(results)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XG-Boost #TODO\n",
    "\n",
    "Let's see if XG-Boost can do any better:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "First let's import the proper packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to select the same predictor features as before and train-test split the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_predictor = df[['condition', 'odometer', 'auto_transmission', 'car_age', 'MSRP', 'body_size']]\n",
    "df_target = df['sellingprice']\n",
    "\n",
    "X_rf_train, X_rf_test, Y_rf_train, Y_rf_test = train_test_split(df_predictor, df_target, test_size=0.3, random_state=39)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=31)\n",
    "rf_model.fit(X_rf_train, Y_rf_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can make predicitons and look at the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Y_rf_pred = rf_model.predict(X_rf_test)\n",
    "\n",
    "mse_rf = mean_squared_error(Y_rf_test, Y_rf_pred)\n",
    "r2_rf = r2_score(Y_rf_test, Y_rf_pred)\n",
    "\n",
    "print(f\"Random Forest MSE: {mse_rf}\")\n",
    "print(f\"Random Forest R^2: {r2_rf}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Model gives us a very strong R^2 value. Let's visualize the actual vs predicted values next. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(Y_rf_test, Y_rf_pred, alpha=0.5, color='blue')\n",
    "plt.plot([min(Y_rf_test), max(Y_rf_test)], [min(Y_rf_test), max(Y_rf_test)], color='red', linewidth=1.0) \n",
    "plt.ylabel('Random Forest Predicted Selling Price')\n",
    "plt.title('Actual vs Random Forest Predicted Selling Price')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "We can also investigate what features were the most important in the splits of the trees. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feat_imp = pd.DataFrame({\n",
    "    'feature': df_predictor.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "feat_imp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feat_imp.set_index('feature').plot(kind = 'bar');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the age of the car was the most important feature to the splitting of the trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results.loc[3] = ['Random Forest Regression', r2_rf]\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What these R-squared values mean is that MMR is the best prediciton for sales price. This makes sense due to MMR being a wholesale market valuation of the car. However, the best model we have built so far is the Random Forest with the highest R-squared value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
